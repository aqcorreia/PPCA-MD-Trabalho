{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f472f96-1794-41f8-aad7-01a8b3c15a9a",
   "metadata": {},
   "source": [
    "## Script que faz a redução de dimensionalidade e clustering\n",
    "\n",
    "Este script utiliza diferentes métodos para **redução de dimensionalidade**, permitindo analisar e visualizar dados de alta dimensão de forma mais compreensível. Os métodos aplicados incluem:\n",
    "\n",
    "- **PCA (Principal Component Analysis)**:  \n",
    "  Método linear que reduz a dimensionalidade preservando a maior parte da variância nos dados. É rápido, simples e eficaz para dados com estrutura linear.\n",
    "\n",
    "- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**:  \n",
    "  Técnica não linear de redução de dimensionalidade, amplamente utilizada para visualização. Prioriza a **preservação de relações locais**, tornando-a ideal para explorar agrupamentos ou padrões em dados complexos.\n",
    "\n",
    "- **KPCA (Kernel Principal Component Analysis)**:  \n",
    "  Extensão não linear do PCA, que utiliza funções kernel para capturar relações complexas nos dados. Útil em cenários onde o PCA tradicional não é suficiente.\n",
    "\n",
    "- **UMAP (Uniform Manifold Approximation and Projection)**:  \n",
    "  Técnica moderna e eficiente que reduz a dimensionalidade **preservando tanto estruturas locais quanto globais**. É rápida, escalável e frequentemente usada para visualização e análise de dados complexos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e07464-a6b1-48b0-9af3-ad50117c16b3",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd4c9ca-7858-492d-a8c1-07932a0184cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhierarchy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dendrogram, linkage\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Biblioteca para redução de dimensionalidade moderna\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\umap\\__init__.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[0;32m      6\u001b[0m         simplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparametric_umap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParametricUMAP, load_ParametricUMAP\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     warn(\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorflow not installed; ParametricUMAP will be unavailable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mImportWarning\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\umap\\parametric_umap.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m torch_imported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\__init__.py:2486\u001b[0m\n\u001b[0;32m   2482\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n\u001b[0;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m-> 2486\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[0;32m   2488\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\_meta_registrations.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     _add_op_to_registry,\n\u001b[0;32m     12\u001b[0m     _convert_out_params,\n\u001b[0;32m     13\u001b[0m     global_decomposition_table,\n\u001b[0;32m     14\u001b[0m     meta_table,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\_decomp\\__init__.py:249\u001b[0m\n\u001b[0;32m    245\u001b[0m             decompositions\u001b[38;5;241m.\u001b[39mpop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_refs\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# See NOTE [Core ATen Ops]\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# list was copied from torch/_inductor/decomposition.py\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# excluding decompositions that results in prim ops\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# Resulting opset of decomposition is core aten ops\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\_decomp\\decompositions.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_meta_registrations\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mprims\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\_prims\\__init__.py:523\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Elementwise unary operations\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m--> 523\u001b[0m \u001b[38;5;28mabs\u001b[39m \u001b[38;5;241m=\u001b[39m _make_elementwise_unary_prim(\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    525\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mabs,\n\u001b[0;32m    526\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    527\u001b[0m     type_promotion\u001b[38;5;241m=\u001b[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mCOMPLEX_TO_FLOAT,\n\u001b[0;32m    528\u001b[0m )\n\u001b[0;32m    530\u001b[0m acos \u001b[38;5;241m=\u001b[39m _make_elementwise_unary_prim(\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macos\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    532\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39macos,\n\u001b[0;32m    533\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    534\u001b[0m     type_promotion\u001b[38;5;241m=\u001b[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[0;32m    535\u001b[0m )\n\u001b[0;32m    537\u001b[0m acosh \u001b[38;5;241m=\u001b[39m _make_elementwise_unary_prim(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macosh\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    539\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39macosh,\n\u001b[0;32m    540\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    541\u001b[0m     type_promotion\u001b[38;5;241m=\u001b[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[0;32m    542\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\_prims\\__init__.py:491\u001b[0m, in \u001b[0;36m_make_elementwise_unary_prim\u001b[1;34m(name, type_promotion, **kwargs)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_elementwise_unary_prim\u001b[39m(\n\u001b[0;32m    485\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, type_promotion: ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    486\u001b[0m ):\n\u001b[0;32m    487\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    Creates an elementwise unary prim.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _make_prim(\n\u001b[0;32m    492\u001b[0m         schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(Tensor self) -> Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    493\u001b[0m         meta\u001b[38;5;241m=\u001b[39mpartial(_prim_elementwise_meta, type_promotion\u001b[38;5;241m=\u001b[39mtype_promotion),\n\u001b[0;32m    494\u001b[0m         return_type\u001b[38;5;241m=\u001b[39mRETURN_TYPE\u001b[38;5;241m.\u001b[39mNEW,\n\u001b[0;32m    495\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    496\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\_prims\\__init__.py:319\u001b[0m, in \u001b[0;36m_make_prim\u001b[1;34m(schema, return_type, meta, impl_aten, doc, tags, use_old_custom_ops_api, register_conj_neg_fallthrough)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg\u001b[38;5;241m.\u001b[39malias_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m arg\u001b[38;5;241m.\u001b[39malias_info\u001b[38;5;241m.\u001b[39mis_write:\n\u001b[0;32m    318\u001b[0m         mutates_args\u001b[38;5;241m.\u001b[39mappend(arg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m--> 319\u001b[0m prim_def \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mcustom_op(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprims::\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name,\n\u001b[0;32m    321\u001b[0m     _prim_impl,\n\u001b[0;32m    322\u001b[0m     mutates_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(mutates_args),\n\u001b[0;32m    323\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m    324\u001b[0m )\n\u001b[0;32m    325\u001b[0m prim_def\u001b[38;5;241m.\u001b[39mregister_fake(meta)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m# all view ops get conj/neg fallthroughs\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\_library\\custom_ops.py:157\u001b[0m, in \u001b[0;36mcustom_op\u001b[1;34m(name, fn, mutates_args, device_types, schema)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner(fn)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\_library\\custom_ops.py:138\u001b[0m, in \u001b[0;36mcustom_op.<locals>.inner\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m    135\u001b[0m     schema_str \u001b[38;5;241m=\u001b[39m schema\n\u001b[0;32m    137\u001b[0m namespace, opname \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 138\u001b[0m result \u001b[38;5;241m=\u001b[39m CustomOpDef(namespace, opname, schema_str, fn)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Check that schema's alias annotations match those of `mutates_args`.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\_library\\custom_ops.py:186\u001b[0m, in \u001b[0;36mCustomOpDef.__init__\u001b[1;34m(self, namespace, name, schema, fn)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vmap_fn: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib \u001b[38;5;241m=\u001b[39m get_library_allowing_overwrite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namespace, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name)\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_to_dispatcher()\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disabled_kernel: Set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    188\u001b[0m OPDEFS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qualname] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\_library\\custom_ops.py:616\u001b[0m, in \u001b[0;36mCustomOpDef._register_to_dispatcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    609\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was no fake impl registered for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    610\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is necessary for torch.compile/export/fx tracing to work. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    611\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.register_fake` to add an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake impl.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m         )\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_abstract_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 616\u001b[0m lib\u001b[38;5;241m.\u001b[39m_register_fake(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, fake_impl, _stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    618\u001b[0m autograd_impl \u001b[38;5;241m=\u001b[39m autograd\u001b[38;5;241m.\u001b[39mmake_autograd_impl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opoverload, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    619\u001b[0m lib\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, autograd_impl, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m, with_keyset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\library.py:163\u001b[0m, in \u001b[0;36mLibrary._register_fake\u001b[1;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_register_fake\u001b[39m(\u001b[38;5;28mself\u001b[39m, op_name, fn, _stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Registers the fake impl for an operator defined in the library.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     source \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_library\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_source(_stacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    164\u001b[0m     frame \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe(_stacklevel)\n\u001b[0;32m    165\u001b[0m     caller_module \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(frame)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\site-packages\\torch\\_library\\utils.py:42\u001b[0m, in \u001b[0;36mget_source\u001b[1;34m(stacklevel)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_source\u001b[39m(stacklevel: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a string that represents the caller.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    Example: \"/path/to/foo.py:42\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    etc.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     frame \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetframeinfo(sys\u001b[38;5;241m.\u001b[39m_getframe(stacklevel))\n\u001b[0;32m     43\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe\u001b[38;5;241m.\u001b[39mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe\u001b[38;5;241m.\u001b[39mlineno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m source\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\inspect.py:1688\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1686\u001b[0m start \u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m context\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m findsource(frame)\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1690\u001b[0m     lines \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\inspect.py:1071\u001b[0m, in \u001b[0;36mfindsource\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (file\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource code not available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1071\u001b[0m module \u001b[38;5;241m=\u001b[39m getmodule(\u001b[38;5;28mobject\u001b[39m, file)\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module:\n\u001b[0;32m   1073\u001b[0m     lines \u001b[38;5;241m=\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetlines(file, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\inspect.py:997\u001b[0m, in \u001b[0;36mgetmodule\u001b[1;34m(object, _filename)\u001b[0m\n\u001b[0;32m    994\u001b[0m         f \u001b[38;5;241m=\u001b[39m getabsfile(module)\n\u001b[0;32m    995\u001b[0m         \u001b[38;5;66;03m# Always map to the name the module knows itself by\u001b[39;00m\n\u001b[0;32m    996\u001b[0m         modulesbyfile[f] \u001b[38;5;241m=\u001b[39m modulesbyfile[\n\u001b[1;32m--> 997\u001b[0m             os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(f)] \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m modulesbyfile:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mget(modulesbyfile[file])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PPCA_MD\\Lib\\ntpath.py:696\u001b[0m, in \u001b[0;36mrealpath\u001b[1;34m(path, strict)\u001b[0m\n\u001b[0;32m    694\u001b[0m     path \u001b[38;5;241m=\u001b[39m join(cwd, path)\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 696\u001b[0m     path \u001b[38;5;241m=\u001b[39m _getfinalpathname(path)\n\u001b[0;32m    697\u001b[0m     initial_winerror \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# gh-106242: Raised for embedded null characters\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;66;03m# In strict mode, we convert into an OSError.\u001b[39;00m\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;66;03m# Non-strict mode returns the path as-is, since we've already\u001b[39;00m\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;66;03m# made it absolute.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Bibliotecas para manipulação de dados e cálculo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time  # Para medir o tempo de execução\n",
    "\n",
    "# Bibliotecas para visualização\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Bibliotecas de aprendizado de máquina\n",
    "from sklearn.manifold import TSNE, LocallyLinearEmbedding\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Bibliotecas para clusterização\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import hdbscan\n",
    "\n",
    "# Métricas para avaliação de clusters\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Hierarquia para dendrogramas\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Biblioteca para redução de dimensionalidade moderna\n",
    "import umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c9856-28b1-4280-922e-8328f1672ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "douItem = 2\n",
    "secao = f'Secao0{douItem}'\n",
    "varEmbedding = \"USE\"\n",
    "varEmbeddingCol = \"4\"\n",
    "# varEmbedding = \"SBERT\"\n",
    "\n",
    "# Lê o arquivo Parquet\n",
    "df_portarias_mgi = pd.read_parquet(f'./saida/04_1_DOU{secao}_portarias_mgi_tratado_NER_lematizado_embeddings{varEmbedding}.parquet', engine='pyarrow')  # ou engine='fastparquet'\n",
    "\n",
    "# Exibe as primeiras linhas do DataFrame\n",
    "df_portarias_mgi.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a324349-6adb-4f4c-a190-ba0263e71c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_portarias_mgi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b684a2-998f-404d-ae5b-699feda52930",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45518e-c661-4a71-b243-6ab445c52900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir o tempo de extração dos embeddings\n",
    "start_time = time.time()\n",
    "\n",
    "# Extrair os embeddings da coluna 'USE_Embeddings' para um array NumPy\n",
    "embeddings = np.vstack(df_portarias_mgi[f'{varEmbedding}_Embeddings{varEmbeddingCol}'])\n",
    "extraction_time = time.time() - start_time\n",
    "print(f\"Tempo para extrair os embeddings: {extraction_time:.2f} segundos\")\n",
    "\n",
    "# Medir o tempo de redução com PCA\n",
    "start_time = time.time()\n",
    "\n",
    "# Reduzir para 2 dimensões com PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_PCA_2d = pca.fit_transform(embeddings)\n",
    "reduction_time = time.time() - start_time\n",
    "print(f\"Tempo para redução dimensional (PCA): {reduction_time:.2f} segundos\")\n",
    "\n",
    "# Plotar os embeddings reduzidos\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_PCA_2d[:, 0], embeddings_PCA_2d[:, 1], s=5)\n",
    "plt.title(f\"Distribuição dos Embeddings (PCA) - {varEmbedding}\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d2f87-2bce-48bd-b64c-0420815d4396",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bccd3-74df-4821-b3b1-c157ae8011b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir o tempo de extração dos embeddings\n",
    "start_time = time.time()\n",
    "\n",
    "# Extrair os embeddings da coluna 'USE_Embeddings' para um array NumPy\n",
    "embeddings = np.vstack(df_portarias_mgi[f'{varEmbedding}_Embeddings{varEmbeddingCol}'])\n",
    "extraction_time = time.time() - start_time\n",
    "print(f\"Tempo para extrair os embeddings: {extraction_time:.2f} segundos\")\n",
    "\n",
    "# Medir o tempo de redução com t-SNE\n",
    "start_time = time.time()\n",
    "# Reduzir para 2 dimensões com t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "embeddings_TSNE_2d = tsne.fit_transform(embeddings)\n",
    "reduction_time = time.time() - start_time\n",
    "print(f\"Tempo para redução dimensional (t-SNE): {reduction_time:.2f} segundos\")\n",
    "\n",
    "# Plotar os embeddings reduzidos\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_TSNE_2d[:, 0], embeddings_TSNE_2d[:, 1], s=5)\n",
    "plt.title(f'Distribuição dos Embeddings (t-SNE) - {varEmbedding}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b07d76-59b9-4801-971f-33e017ab8505",
   "metadata": {},
   "source": [
    "## Kpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7cabf-827d-48f1-a21b-f58b3f238b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir o tempo de extração dos embeddings\n",
    "start_time = time.time()\n",
    "\n",
    "# Extrair os embeddings da coluna 'USE_Embeddings' para um array NumPy\n",
    "embeddings = np.vstack(df_portarias_mgi[f'{varEmbedding}_Embeddings{varEmbeddingCol}'])\n",
    "extraction_time = time.time() - start_time\n",
    "print(f\"Tempo para extrair os embeddings: {extraction_time:.2f} segundos\")\n",
    "\n",
    "# Medir o tempo de redução com KPCA\n",
    "start_time = time.time()\n",
    "\n",
    "# Reduzir para 2 dimensões com Kernel PCA\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf', gamma=0.1)  # Usando kernel RBF\n",
    "embeddings_kpca_2d = kpca.fit_transform(embeddings)\n",
    "reduction_time = time.time() - start_time\n",
    "print(f\"Tempo para redução dimensional (KPCA): {reduction_time:.2f} segundos\")\n",
    "\n",
    "# Plotar os embeddings reduzidos\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_kpca_2d[:, 0], embeddings_kpca_2d[:, 1], s=5)\n",
    "plt.title(f'Distribuição dos Embeddings (KPCA) - {varEmbedding}')\n",
    "plt.xlabel(\"Componente Kernel 1\")\n",
    "plt.ylabel(\"Componente Kernel 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd5445e-578c-4061-86c1-dc925fbe49fd",
   "metadata": {},
   "source": [
    "## PCA e t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab0195-1747-4b7e-8db7-75a74d5e2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir o tempo de extração dos embeddings\n",
    "start_time = time.time()\n",
    "\n",
    "# Extrair os embeddings da coluna 'USE_Embeddings' para um array NumPy\n",
    "embeddings = np.vstack(df_portarias_mgi[f'{varEmbedding}_Embeddings{varEmbeddingCol}'])\n",
    "extraction_time = time.time() - start_time\n",
    "print(f\"Tempo para extrair os embeddings: {extraction_time:.2f} segundos\")\n",
    "\n",
    "# Etapa 1: Reduzir dimensionalidade com PCA\n",
    "start_time = time.time()\n",
    "pca = PCA(n_components=50)  # Reduzir para 50 dimensões intermediárias\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "pca_time = time.time() - start_time\n",
    "print(f\"Tempo para redução dimensional (PCA): {pca_time:.2f} segundos\")\n",
    "\n",
    "# Etapa 2: Reduzir ainda mais com t-SNE\n",
    "start_time = time.time()\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "embeddings_pca_tsne = tsne.fit_transform(embeddings_pca)\n",
    "tsne_time = time.time() - start_time\n",
    "print(f\"Tempo para redução dimensional (t-SNE): {tsne_time:.2f} segundos\")\n",
    "\n",
    "# Plotar os embeddings reduzidos\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_pca_tsne[:, 0], embeddings_pca_tsne[:, 1], s=5)\n",
    "plt.title(f'Distribuição dos Embeddings (PCA + t-SNE) - {varEmbedding}')\n",
    "plt.xlabel(\"Dimensão 1 (t-SNE)\")\n",
    "plt.ylabel(\"Dimensão 2 (t-SNE)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18be06-44aa-487e-91f9-d2ba74d2d4b7",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43f21c-51e5-4c7a-9270-c65819243aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir o tempo de extração dos embeddings\n",
    "start_time = time.time()\n",
    "\n",
    "# Extrair os embeddings da coluna 'USE_Embeddings' para um array NumPy\n",
    "embeddings = np.vstack(df_portarias_mgi[f'{varEmbedding}_Embeddings{varEmbeddingCol}'])\n",
    "extraction_time = time.time() - start_time\n",
    "print(f\"Tempo para extrair os embeddings: {extraction_time:.2f} segundos\")\n",
    "\n",
    "# Medir o tempo de redução com UMAP\n",
    "start_time = time.time()\n",
    "\n",
    "# Reduzir para 2 dimensões com UMAP\n",
    "umap_model = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "embeddings_umap_2d = umap_model.fit_transform(embeddings)\n",
    "reduction_time = time.time() - start_time\n",
    "print(f\"Tempo para redução dimensional (UMAP): {reduction_time:.2f} segundos\")\n",
    "\n",
    "# Plotar os embeddings reduzidos\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_umap_2d[:, 0], embeddings_umap_2d[:, 1], s=5)\n",
    "plt.title(f'Distribuição dos Embeddings (UMAP) - {varEmbedding}')\n",
    "plt.xlabel(\"Componente UMAP 1\")\n",
    "plt.ylabel(\"Componente UMAP 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ace3e31-35ba-448f-bafd-33c6c37f8ba2",
   "metadata": {},
   "source": [
    "## t-SNE + K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203bd686-ed2b-41fd-a175-87d022098a24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Medir o tempo de execução do K-Means\n",
    "start_time = time.time()\n",
    "\n",
    "# Aplicar K-Means nos dados reduzidos\n",
    "n_clusters = 4  # Definir o número de clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(embeddings_TSNE_2d)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"Tempo de execução do K-Means: {execution_time:.2f} segundos \")\n",
    "print(f'Clusters = {n_clusters}')\n",
    "\n",
    "# Validar a clusterização com métricas não supervisionadas\n",
    "silhouette_avg = silhouette_score(embeddings_TSNE_2d, kmeans.labels_)\n",
    "davies_bouldin = davies_bouldin_score(embeddings_TSNE_2d, kmeans.labels_)\n",
    "calinski_harabasz = calinski_harabasz_score(embeddings_TSNE_2d, kmeans.labels_)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin:.2f}\")\n",
    "print(f\"Calinski-Harabasz Score: {calinski_harabasz:.2f}\")\n",
    "\n",
    "# Obter os clusters\n",
    "df_portarias_mgi['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Visualizar os clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_TSNE_2d[:, 0], embeddings_TSNE_2d[:, 1], c=kmeans.labels_, cmap='viridis', s=5, label='Dados')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=100, marker='X', label='Centróides')\n",
    "plt.title(f'Clusters Criados pelo K-Means após t-SNE - {varEmbedding}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2262c-0303-4230-995c-6fc93c83ff38",
   "metadata": {},
   "source": [
    "## PCA + t-SNE + K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2c348-e3b5-4ef2-ab96-dd58e934337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir o tempo de execução do K-Means\n",
    "start_time = time.time()\n",
    "\n",
    "# Aplicar K-Means nos dados reduzidos\n",
    "n_clusters = 4  # Definir o número de clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(embeddings_pca_tsne)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"Tempo de execução do K-Means: {execution_time:.2f} segundos\")\n",
    "print(f'Clusters = {n_clusters}')\n",
    "\n",
    "# Validar a clusterização com métricas não supervisionadas\n",
    "silhouette_avg = silhouette_score(embeddings_pca_tsne, kmeans.labels_)\n",
    "davies_bouldin = davies_bouldin_score(embeddings_pca_tsne, kmeans.labels_)\n",
    "calinski_harabasz = calinski_harabasz_score(embeddings_pca_tsne, kmeans.labels_)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin:.2f}\")\n",
    "print(f\"Calinski-Harabasz Score: {calinski_harabasz:.2f}\")\n",
    "\n",
    "# Obter os clusters\n",
    "df_portarias_mgi['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Visualizar os clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_pca_tsne[:, 0], embeddings_pca_tsne[:, 1], c=kmeans.labels_, cmap='viridis', s=5, label='Dados')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=100, marker='X', label='Centróides')\n",
    "plt.title(f'Clusters Criados pelo K-Means após PCA + t-SNE - {varEmbedding}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf67500-2d04-449c-8ccb-6c6f64a8e5a8",
   "metadata": {},
   "source": [
    "## UMAP + K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc37972-f3f1-4537-b343-70a489e6ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir o tempo de execução do K-Means\n",
    "start_time = time.time()\n",
    "\n",
    "# Aplicar K-Means nos dados reduzidos\n",
    "n_clusters = 6  # Definir o número de clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(embeddings_umap_2d)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"Tempo de execução do K-Means: {execution_time:.2f} segundos\")\n",
    "print(f'Clusters = {n_clusters}')\n",
    "\n",
    "# Validar a clusterização com métricas não supervisionadas\n",
    "silhouette_avg = silhouette_score(embeddings_umap_2d, kmeans.labels_)\n",
    "davies_bouldin = davies_bouldin_score(embeddings_umap_2d, kmeans.labels_)\n",
    "calinski_harabasz = calinski_harabasz_score(embeddings_umap_2d, kmeans.labels_)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin:.2f}\")\n",
    "print(f\"Calinski-Harabasz Score: {calinski_harabasz:.2f}\")\n",
    "\n",
    "# Obter os clusters\n",
    "df_portarias_mgi['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Visualizar os clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_umap_2d[:, 0], embeddings_umap_2d[:, 1], c=kmeans.labels_, cmap='viridis', s=5, label='Dados')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=100, marker='X', label='Centróides')\n",
    "plt.title(f'Clusters Criados pelo K-Means após UMAP - {varEmbedding}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df652e89-ccad-4e4f-b95a-9ffc5c8bc7d8",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be241c7f-4728-4fa9-9ff5-a88ef8886c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o número de clusters\n",
    "n_clusters = 6\n",
    "\n",
    "# Aplicação do Hierarchical Clustering (Agglomerative)\n",
    "hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "labels = hierarchical.fit_predict(embeddings_pca_tsne)\n",
    "\n",
    "# Calcular métricas de clusterização\n",
    "if len(set(labels)) > 1:  # Garantir que há mais de um cluster\n",
    "    silhouette_avg = silhouette_score(embeddings_pca_tsne, labels)\n",
    "    davies_bouldin = davies_bouldin_score(embeddings_pca_tsne, labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(embeddings_pca_tsne, labels)\n",
    "\n",
    "    print(f\"Índice de Silhueta: {silhouette_avg:.4f}\")\n",
    "    print(f\"Índice de Davies-Bouldin: {davies_bouldin:.4f}\")\n",
    "    print(f\"Índice de Calinski-Harabasz: {calinski_harabasz:.4f}\")\n",
    "else:\n",
    "    print(\"Não há clusters suficientes para calcular as métricas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ecffb1-ed83-429f-9c1c-7a28c4acf72f",
   "metadata": {},
   "source": [
    "## t-NSE + DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b1ea1-95e1-4d69-a447-c52369b246e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicação do DBSCAN nos dados reduzidos\n",
    "dbscan = DBSCAN(eps=0.7, min_samples=3, metric='euclidean')\n",
    "clusters = dbscan.fit_predict(embeddings_TSNE_2d)\n",
    "\n",
    "# Adicionar clusters ao dataframe\n",
    "df_portarias_mgi[f'Cluster_tnse_dbscan_{varEmbedding}'] = clusters\n",
    "\n",
    "# Filtrar os dados para clusters válidos (excluindo ruído)\n",
    "mask_valid_clusters = clusters != -1  # Ignorar rótulos de ruído (-1)\n",
    "valid_embeddings = embeddings_TSNE_2d[mask_valid_clusters]\n",
    "valid_clusters = clusters[mask_valid_clusters]\n",
    "\n",
    "# Verificar se há clusters válidos suficientes\n",
    "if len(set(valid_clusters)) > 1:  # Garantir que há mais de um cluster válido\n",
    "    # Índice de Silhueta\n",
    "    silhouette_avg = silhouette_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Silhueta (clusters válidos): {silhouette_avg:.4f}\")\n",
    "\n",
    "    # Índice de Davies-Bouldin\n",
    "    davies_bouldin = davies_bouldin_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Davies-Bouldin (clusters válidos): {davies_bouldin:.4f}\")\n",
    "\n",
    "    # Índice de Calinski-Harabasz\n",
    "    calinski_harabasz = calinski_harabasz_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Calinski-Harabasz (clusters válidos): {calinski_harabasz:.4f}\")\n",
    "else:\n",
    "    print(\"Não há clusters suficientes para calcular as métricas.\")\n",
    "\n",
    "num_points = len(clusters)\n",
    "num_noise = np.sum(clusters == -1)\n",
    "noise_ratio = num_noise / num_points\n",
    "print(f\"Proporção de ruído: {noise_ratio:.2%}\")\n",
    "\n",
    "# Total de clusters válidos (excluindo ruído)\n",
    "total_clusters_excl_noise = len(set(clusters) - {-1})  # Remove o cluster de ruído (-1)\n",
    "print(f\"Total de clusters válidos (excluindo ruído): {total_clusters_excl_noise}\")\n",
    "\n",
    "# Visualização dos clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_TSNE_2d[:, 0], embeddings_TSNE_2d[:, 1], c=clusters, cmap='viridis', s=5)\n",
    "plt.title(f\"Clusters com t-SNE + DBSCAN - {varEmbedding}\")\n",
    "plt.xlabel(\"Dimensão 1\")\n",
    "plt.ylabel(\"Dimensão 2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c75d9e-41cc-4fdf-9be9-7226dc677fa9",
   "metadata": {},
   "source": [
    "## UMAP + DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5d1d6-46bc-4b64-9200-c244d7d63f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicação do DBSCAN nos dados reduzidos\n",
    "dbscan = DBSCAN(eps=0.7, min_samples=3, metric='euclidean')\n",
    "clusters = dbscan.fit_predict(embeddings_umap_2d)\n",
    "\n",
    "# Adicionar clusters ao dataframe\n",
    "df_portarias_mgi[f'Cluster_umap_dbscan_{varEmbedding}'] = clusters\n",
    "\n",
    "# Filtrar os dados para clusters válidos (excluindo ruído)\n",
    "mask_valid_clusters = clusters != -1  # Ignorar rótulos de ruído (-1)\n",
    "valid_embeddings = embeddings_umap_2d[mask_valid_clusters]\n",
    "valid_clusters = clusters[mask_valid_clusters]\n",
    "\n",
    "# Verificar se há clusters válidos suficientes\n",
    "if len(set(valid_clusters)) > 1:  # Garantir que há mais de um cluster válido\n",
    "    # Índice de Silhueta\n",
    "    silhouette_avg = silhouette_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Silhueta (clusters válidos): {silhouette_avg:.4f}\")\n",
    "\n",
    "    # Índice de Davies-Bouldin\n",
    "    davies_bouldin = davies_bouldin_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Davies-Bouldin (clusters válidos): {davies_bouldin:.4f}\")\n",
    "\n",
    "    # Índice de Calinski-Harabasz\n",
    "    calinski_harabasz = calinski_harabasz_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Calinski-Harabasz (clusters válidos): {calinski_harabasz:.4f}\")\n",
    "else:\n",
    "    print(\"Não há clusters suficientes para calcular as métricas.\")\n",
    "\n",
    "num_points = len(clusters)\n",
    "num_noise = np.sum(clusters == -1)\n",
    "noise_ratio = num_noise / num_points\n",
    "print(f\"Proporção de ruído: {noise_ratio:.2%}\")\n",
    "\n",
    "# Total de clusters válidos (excluindo ruído)\n",
    "total_clusters_excl_noise = len(set(clusters) - {-1})  # Remove o cluster de ruído (-1)\n",
    "print(f\"Total de clusters válidos (excluindo ruído): {total_clusters_excl_noise}\")\n",
    "\n",
    "# Visualização dos clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_umap_2d[:, 0], embeddings_umap_2d[:, 1], c=clusters, cmap='viridis', s=5)\n",
    "plt.title(f\"Clusters com UMAP + DBSCAN - {varEmbedding}\")\n",
    "plt.xlabel(\"Dimensão 1\")\n",
    "plt.ylabel(\"Dimensão 2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29027b46-b457-48f9-8bc0-321e32db4637",
   "metadata": {},
   "source": [
    "## t-NSE + HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a2487-8ead-4207-9935-a7abebf0f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar HDBSCAN\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=25, min_samples=6, metric='euclidean')\n",
    "clusters = hdbscan_clusterer.fit_predict(embeddings_TSNE_2d)\n",
    "\n",
    "# Adicionar clusters ao DataFrame\n",
    "df_portarias_mgi[f'Cluster_tnse_hdbscan_{varEmbedding}'] = clusters\n",
    "\n",
    "# Filtrar os dados para clusters válidos (excluindo ruído)\n",
    "mask_valid_clusters = clusters != -1  # Ignorar rótulos de ruído (-1)\n",
    "valid_embeddings = embeddings_TSNE_2d[mask_valid_clusters]\n",
    "valid_clusters = clusters[mask_valid_clusters]\n",
    "\n",
    "# Verificar se há clusters válidos suficientes\n",
    "if len(set(valid_clusters)) > 1:  # Garantir que há mais de um cluster válido\n",
    "    # Índice de Silhueta\n",
    "    silhouette_avg = silhouette_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Silhueta (clusters válidos): {silhouette_avg:.4f}\")\n",
    "\n",
    "    # Índice de Davies-Bouldin\n",
    "    davies_bouldin = davies_bouldin_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Davies-Bouldin (clusters válidos): {davies_bouldin:.4f}\")\n",
    "\n",
    "    # Índice de Calinski-Harabasz\n",
    "    calinski_harabasz = calinski_harabasz_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Calinski-Harabasz (clusters válidos): {calinski_harabasz:.4f}\")\n",
    "else:\n",
    "    print(\"Não há clusters suficientes para calcular as métricas.\")\n",
    "\n",
    "# Proporção de ruído\n",
    "num_points = len(clusters)\n",
    "num_noise = np.sum(clusters == -1)\n",
    "noise_ratio = num_noise / num_points\n",
    "print(f\"Proporção de ruído: {noise_ratio:.2%}\")\n",
    "\n",
    "# Total de clusters válidos (excluindo ruído)\n",
    "total_clusters_excl_noise = len(set(clusters) - {-1})  # Remove o cluster de ruído (-1)\n",
    "print(f\"Total de clusters válidos (excluindo ruído): {total_clusters_excl_noise}\")\n",
    "\n",
    "# Visualização dos clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    embeddings_TSNE_2d[:, 0], embeddings_TSNE_2d[:, 1], \n",
    "    c=clusters, cmap='viridis', s=5\n",
    ")\n",
    "plt.title(f\"Clusters com t-SNE + HDBSCAN - {varEmbedding}\")\n",
    "plt.xlabel(\"Dimensão 1\")\n",
    "plt.ylabel(\"Dimensão 2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0492d0f2-6d5c-4871-a1fd-cc944ffed81e",
   "metadata": {},
   "source": [
    "## UMAP + HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc2616-d1f9-4210-9dd6-500a492f9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar HDBSCAN\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=25, min_samples=6, metric='euclidean')\n",
    "clusters = hdbscan_clusterer.fit_predict(embeddings_umap_2d)\n",
    "\n",
    "# Adicionar clusters ao DataFrame\n",
    "df_portarias_mgi[f'Cluster_umap_hdbscan_{varEmbedding}'] = clusters\n",
    "\n",
    "# Filtrar os dados para clusters válidos (excluindo ruído)\n",
    "mask_valid_clusters = clusters != -1  # Ignorar rótulos de ruído (-1)\n",
    "valid_embeddings = embeddings_TSNE_2d[mask_valid_clusters]\n",
    "valid_clusters = clusters[mask_valid_clusters]\n",
    "\n",
    "# Verificar se há clusters válidos suficientes\n",
    "if len(set(valid_clusters)) > 1:  # Garantir que há mais de um cluster válido\n",
    "    # Índice de Silhueta\n",
    "    silhouette_avg = silhouette_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Silhueta (clusters válidos): {silhouette_avg:.4f}\")\n",
    "\n",
    "    # Índice de Davies-Bouldin\n",
    "    davies_bouldin = davies_bouldin_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Davies-Bouldin (clusters válidos): {davies_bouldin:.4f}\")\n",
    "\n",
    "    # Índice de Calinski-Harabasz\n",
    "    calinski_harabasz = calinski_harabasz_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Calinski-Harabasz (clusters válidos): {calinski_harabasz:.4f}\")\n",
    "else:\n",
    "    print(\"Não há clusters suficientes para calcular as métricas.\")\n",
    "\n",
    "# Proporção de ruído\n",
    "num_points = len(clusters)\n",
    "num_noise = np.sum(clusters == -1)\n",
    "noise_ratio = num_noise / num_points\n",
    "print(f\"Proporção de ruído: {noise_ratio:.2%}\")\n",
    "\n",
    "# Total de clusters válidos (excluindo ruído)\n",
    "total_clusters_excl_noise = len(set(clusters) - {-1})  # Remove o cluster de ruído (-1)\n",
    "print(f\"Total de clusters válidos (excluindo ruído): {total_clusters_excl_noise}\")\n",
    "\n",
    "# Visualização dos clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    embeddings_umap_2d[:, 0], embeddings_umap_2d[:, 1], \n",
    "    c=clusters, cmap='viridis', s=5\n",
    ")\n",
    "plt.title(f\"Clusters com UMAP + HDBSCAN - {varEmbedding}\")\n",
    "plt.xlabel(\"Dimensão 1\")\n",
    "plt.ylabel(\"Dimensão 2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94455ab3-d3a0-4339-84fb-7128056b0187",
   "metadata": {},
   "source": [
    "## PCA + t-NSE + DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad42d6-7ef1-4b8c-948d-9bf92cd40260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicação do DBSCAN nos dados reduzidos\n",
    "dbscan = DBSCAN(eps=0.7, min_samples=3, metric='euclidean')\n",
    "clusters = dbscan.fit_predict(embeddings_TSNE_2d)\n",
    "\n",
    "# Adicionar clusters ao dataframe\n",
    "df_portarias_mgi[f'Cluster_pca_tnse_dbscan_{varEmbedding}'] = clusters\n",
    "\n",
    "# Filtrar os dados para clusters válidos (excluindo ruído)\n",
    "mask_valid_clusters = clusters != -1  # Ignorar rótulos de ruído (-1)\n",
    "valid_embeddings = embeddings_pca_tsne[mask_valid_clusters]\n",
    "valid_clusters = clusters[mask_valid_clusters]\n",
    "\n",
    "# Verificar se há clusters válidos suficientes\n",
    "if len(set(valid_clusters)) > 1:  # Garantir que há mais de um cluster válido\n",
    "    # Índice de Silhueta\n",
    "    silhouette_avg = silhouette_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Silhueta (clusters válidos): {silhouette_avg:.4f}\")\n",
    "\n",
    "    # Índice de Davies-Bouldin\n",
    "    davies_bouldin = davies_bouldin_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Davies-Bouldin (clusters válidos): {davies_bouldin:.4f}\")\n",
    "\n",
    "    # Índice de Calinski-Harabasz\n",
    "    calinski_harabasz = calinski_harabasz_score(valid_embeddings, valid_clusters)\n",
    "    print(f\"Índice de Calinski-Harabasz (clusters válidos): {calinski_harabasz:.4f}\")\n",
    "else:\n",
    "    print(\"Não há clusters suficientes para calcular as métricas.\")\n",
    "\n",
    "num_points = len(clusters)\n",
    "num_noise = np.sum(clusters == -1)\n",
    "noise_ratio = num_noise / num_points\n",
    "print(f\"Proporção de ruído: {noise_ratio:.2%}\")\n",
    "\n",
    "# Total de clusters válidos (excluindo ruído)\n",
    "total_clusters_excl_noise = len(set(clusters) - {-1})  # Remove o cluster de ruído (-1)\n",
    "print(f\"Total de clusters válidos (excluindo ruído): {total_clusters_excl_noise}\")\n",
    "\n",
    "# Visualização dos clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_pca_tsne[:, 0], embeddings_pca_tsne[:, 1], c=clusters, cmap='viridis', s=5)\n",
    "plt.title(f\"Clusters com PCA + t-SNE + DBSCAN - {varEmbedding}\")\n",
    "plt.xlabel(\"Dimensão 1\")\n",
    "plt.ylabel(\"Dimensão 2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff202ec-c410-4f63-9180-8ee3e91b9c73",
   "metadata": {},
   "source": [
    "## t-SNE + GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f6fb8-1910-4a19-816a-e9655e26e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de clusters a ser testado\n",
    "n_components = 4\n",
    "\n",
    "# Aplicar Gaussian Mixture Models (GMM)\n",
    "gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "labels = gmm.fit_predict(embeddings_TSNE_2d)\n",
    "\n",
    "# Calcular métricas de clusterização\n",
    "if len(set(labels)) > 1:  # Garantir que há mais de um cluster\n",
    "    silhouette_avg = silhouette_score(embeddings_TSNE_2d, labels)\n",
    "    davies_bouldin = davies_bouldin_score(embeddings_TSNE_2d, labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(embeddings_TSNE_2d, labels)\n",
    "\n",
    "    print(f\"Índice de Silhueta: {silhouette_avg:.4f}\")\n",
    "    print(f\"Índice de Davies-Bouldin: {davies_bouldin:.4f}\")\n",
    "    print(f\"Índice de Calinski-Harabasz: {calinski_harabasz:.4f}\")\n",
    "else:\n",
    "    print(\"Não há clusters suficientes para calcular as métricas.\")\n",
    "\n",
    "# Visualização dos clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings_TSNE_2d[:, 0], embeddings_TSNE_2d[:, 1], c=labels, cmap='viridis', s=10)\n",
    "plt.title(f\"Clusters com t-SNE + GMM (n_components={n_components}) - {varEmbedding}\")\n",
    "plt.xlabel(\"Dimensão 1\")\n",
    "plt.ylabel(\"Dimensão 2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec32808-e6fc-4e64-aeda-c42a0d6b7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular as distâncias ao k-vizinho mais próximo (n_neighbors = min_samples)\n",
    "neighbors = NearestNeighbors(n_neighbors=5)  # Use o mesmo valor de min_samples\n",
    "neighbors_fit = neighbors.fit(embeddings_TSNE_2d)\n",
    "distances, indices = neighbors_fit.kneighbors(embeddings_scaled)\n",
    "\n",
    "# Ordenar as distâncias e plotar\n",
    "distances = np.sort(distances[:, 4])  # Distância ao 5º vizinho\n",
    "plt.plot(distances)\n",
    "plt.title(\"Gráfico de Distância k-Vizinha\")\n",
    "plt.xlabel(\"Pontos\")\n",
    "plt.ylabel(\"Distância ao 5º Vizinho\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d30ace-6c16-4d10-88c7-12e6a2182ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_portarias_mgi['Cluster'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ebd88-b605-4c93-9f7a-4d545dbc2da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_portarias_mgi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331087ff-75a3-49c1-b0fe-04fe7bc5543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar e contar registros distintos em cada cluster\n",
    "total_por_cluster = df_portarias_mgi.groupby('Cluster_UMAP_DBSCAN').size().reset_index(name='Total')\n",
    "\n",
    "# Exibir o resultado\n",
    "total_por_cluster.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92cdf3b-8a89-40b5-ab29-ddb101b93b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
